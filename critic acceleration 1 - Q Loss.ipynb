{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pybullet_envs\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.optim import Adam\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import dmc2gym\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20\n",
    "\n",
    "class SquashedGaussianMLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)\n",
    "        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs, deterministic=False, with_logprob=True):\n",
    "        net_out = self.net(obs)\n",
    "        mu = self.mu_layer(net_out)\n",
    "        log_std = self.log_std_layer(net_out)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # Pre-squash distribution and sample\n",
    "        pi_distribution = Normal(mu, std)\n",
    "        if deterministic:\n",
    "            # Only used for evaluating policy at test time.\n",
    "            pi_action = mu\n",
    "        else:\n",
    "            pi_action = pi_distribution.rsample()\n",
    "\n",
    "        if with_logprob:\n",
    "            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n",
    "            # NOTE: The correction formula is a little bit magic. To get an understanding \n",
    "            # of where it comes from, check out the original SAC paper (arXiv 1801.01290) \n",
    "            # and look in appendix C. This is a more numerically-stable equivalent to Eq 21.\n",
    "            # Try deriving it yourself as a (very difficult) exercise. :)\n",
    "            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n",
    "            logp_pi -= (2*(np.log(2) - pi_action - F.softplus(-2*pi_action))).sum(axis=1)\n",
    "        else:\n",
    "            logp_pi = None\n",
    "\n",
    "        pi_action = torch.tanh(pi_action)\n",
    "        pi_action = self.act_limit * pi_action\n",
    "\n",
    "        return pi_action, logp_pi\n",
    "\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
    "        self.q1 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "        self.q2 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "\n",
    "    def act(self, obs, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            a, _ = self.pi(obs, deterministic, False)\n",
    "            return a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(o, deterministic=False):\n",
    "    #return test_env.action_space.sample()\n",
    "    return ac.act(T.as_tensor(o, dtype=T.float32), deterministic)\n",
    "\n",
    "seeds = ['123', '666', '742', '637', '4637']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_env = dmc2gym.make(domain_name='walker', task_name='stand')\n",
    "print(test_env.observation_space.shape, test_env.action_space.shape)\n",
    "\n",
    "for i in range(0,5):\n",
    "    dir_path = \"data/sac_dmWalkerStand_256_retroloss_2/sac_dmWalkerStand_256_retroloss_2_s\" + seeds[i]\n",
    "\n",
    "    ac = T.load(dir_path+\"/pyt_save/model.pt\")\n",
    "\n",
    "    ac.eval()\n",
    "\n",
    "    max_ep_len = 1000\n",
    "\n",
    "    trajs = []\n",
    "\n",
    "    for i in range(100):\n",
    "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "        while not(d or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time \n",
    "            prev_o = o\n",
    "            action = get_action(o, True)\n",
    "            o, r, d, _ = test_env.step(action)\n",
    "            trajs.append([prev_o, action, o])\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "        print(ep_ret)\n",
    "    with open(dir_path+'/demos.pkl','wb') as f:\n",
    "        pickle.dump(trajs, f)\n",
    "\n",
    "#print(trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,5):\n",
    "    dir_path = \"data/sac_dmWalkerStand_256_retroloss_2/sac_dmWalkerStand_256_retroloss_2_s\" + seeds[k]\n",
    "    trajs_final = pickle.load(open(dir_path+'/demos.pkl', 'rb'))\n",
    "    \n",
    "    obss = []\n",
    "    acts = []\n",
    "    for traj in trajs_final:\n",
    "        obs = traj[0]\n",
    "        act = traj[1]\n",
    "        obss.append(obs)\n",
    "        acts.append(act)\n",
    "\n",
    "    obss = np.array(obss)\n",
    "    acts = np.array(acts)\n",
    "\n",
    "    idxs = np.random.randint(0, len(obss), size=500)\n",
    "    batch = dict(obss_sample=obss[idxs], acts_sample=acts[idxs])\n",
    "    data = {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}\n",
    "    \n",
    "    o, a = data['obss_sample'], data['acts_sample']\n",
    "    q1 = ac.q1(o,a)\n",
    "    q2 = ac.q2(o,a)\n",
    "    \n",
    "    ac_kwargs = {}\n",
    "    lr = 0.001\n",
    "\n",
    "    dir_path2 = \"data/sac_dmWalkerStand_256_retroloss_2/sac_dmWalkerStand_256_retroloss_2_s\" + seeds[k]\n",
    "\n",
    "    q_losses = []\n",
    "    \n",
    "    \n",
    "    i = 10000\n",
    "    while True:\n",
    "        print(i)\n",
    "        if i==2000000:\n",
    "            break\n",
    "\n",
    "        ac2 = MLPActorCritic(test_env.observation_space, test_env.action_space, **ac_kwargs)\n",
    "        pi_optimizer = Adam(ac2.pi.parameters(), lr=lr)\n",
    "        q_params = itertools.chain(ac2.q1.parameters(), ac2.q2.parameters())\n",
    "        q_optimizer = Adam(q_params, lr=lr)\n",
    "\n",
    "        model_path = \"/checkpoints/model_checkpoint_{}.tar\".format(i)\n",
    "        checkpoint = T.load(dir_path2+model_path)\n",
    "        ac2.load_state_dict(checkpoint['ac_state_dict'])\n",
    "        pi_optimizer.load_state_dict(checkpoint['pi_optimizer_state_dict'])\n",
    "        q_optimizer.load_state_dict(checkpoint['q_optimizer_state_dict'])\n",
    "        ac2.eval()\n",
    "\n",
    "        q1_2 = ac2.q1(o,a)\n",
    "        q2_2 = ac2.q2(o,a)\n",
    "\n",
    "        loss_q1 = ((q1 - q1_2)**2).mean()\n",
    "        loss_q2 = ((q2 - q2_2)**2).mean()\n",
    "        loss_q = loss_q1 + loss_q2\n",
    "\n",
    "        print(loss_q.item())\n",
    "        q_losses.append(loss_q.item())\n",
    "        i = i+10000\n",
    "    with open(dir_path2 +'/q_losses.pkl','wb') as f:\n",
    "        pickle.dump(q_losses, f)\n",
    "        \n",
    "        \n",
    "    dir_path3 = \"data/sac_dmWalkerStand_256/sac_dmWalkerStand_256_s\" + seeds[k]\n",
    "\n",
    "    q_losses = []\n",
    "    \n",
    "    \n",
    "    i = 10000\n",
    "    while True:\n",
    "        print(i)\n",
    "        if i==2000000:\n",
    "            break\n",
    "\n",
    "        ac2 = MLPActorCritic(test_env.observation_space, test_env.action_space, **ac_kwargs)\n",
    "        pi_optimizer = Adam(ac2.pi.parameters(), lr=lr)\n",
    "        q_params = itertools.chain(ac2.q1.parameters(), ac2.q2.parameters())\n",
    "        q_optimizer = Adam(q_params, lr=lr)\n",
    "\n",
    "        model_path = \"/checkpoints/model_checkpoint_{}.tar\".format(i)\n",
    "        checkpoint = T.load(dir_path3+model_path)\n",
    "        ac2.load_state_dict(checkpoint['ac_state_dict'])\n",
    "        pi_optimizer.load_state_dict(checkpoint['pi_optimizer_state_dict'])\n",
    "        q_optimizer.load_state_dict(checkpoint['q_optimizer_state_dict'])\n",
    "        ac2.eval()\n",
    "\n",
    "        q1_2 = ac2.q1(o,a)\n",
    "        q2_2 = ac2.q2(o,a)\n",
    "\n",
    "        loss_q1 = ((q1 - q1_2)**2).mean()\n",
    "        loss_q2 = ((q2 - q2_2)**2).mean()\n",
    "        loss_q = loss_q1 + loss_q2\n",
    "\n",
    "        print(loss_q.item())\n",
    "        q_losses.append(loss_q.item())\n",
    "        i = i+10000\n",
    "    with open(dir_path3+'/q_losses.pkl','wb') as f:\n",
    "        pickle.dump(q_losses, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
