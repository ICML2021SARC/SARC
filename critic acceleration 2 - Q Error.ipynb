{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pybullet_envs\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.optim import Adam\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import dmc2gym\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20\n",
    "\n",
    "class SquashedGaussianMLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)\n",
    "        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs, deterministic=False, with_logprob=True):\n",
    "        net_out = self.net(obs)\n",
    "        mu = self.mu_layer(net_out)\n",
    "        log_std = self.log_std_layer(net_out)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # Pre-squash distribution and sample\n",
    "        pi_distribution = Normal(mu, std)\n",
    "        if deterministic:\n",
    "            # Only used for evaluating policy at test time.\n",
    "            pi_action = mu\n",
    "        else:\n",
    "            pi_action = pi_distribution.rsample()\n",
    "\n",
    "        if with_logprob:\n",
    "            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n",
    "            # NOTE: The correction formula is a little bit magic. To get an understanding \n",
    "            # of where it comes from, check out the original SAC paper (arXiv 1801.01290) \n",
    "            # and look in appendix C. This is a more numerically-stable equivalent to Eq 21.\n",
    "            # Try deriving it yourself as a (very difficult) exercise. :)\n",
    "            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n",
    "            logp_pi -= (2*(np.log(2) - pi_action - F.softplus(-2*pi_action))).sum(axis=1)\n",
    "        else:\n",
    "            logp_pi = None\n",
    "\n",
    "        pi_action = torch.tanh(pi_action)\n",
    "        pi_action = self.act_limit * pi_action\n",
    "\n",
    "        return pi_action, logp_pi\n",
    "\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
    "        self.q1 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "        self.q2 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "\n",
    "    def act(self, obs, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            a, _ = self.pi(obs, deterministic, False)\n",
    "            return a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = ['123', '666', '742', '637', '4637']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_env = dmc2gym.make(domain_name='walker', task_name='stand')\n",
    "for k in range(0,5):\n",
    "    \n",
    "    ac_kwargs = {}\n",
    "    lr = 0.001\n",
    "\n",
    "    dir_path = \"data/sac_dmWalkerStand_256/sac_dmWalkerStand_256_s\" + seeds[k]\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Checkpoint', 'Return', 'Undiscounted Return', 'Soft Return', 'Q1',  \n",
    "                                                  'Q2', 'AvgQ', 'MSE', 'MAE']) \n",
    "    \n",
    "    i = 10000\n",
    "    while True:\n",
    "        print(i)\n",
    "        \n",
    "        if i==2000000:\n",
    "            ac2 = T.load(dir_path+\"/pyt_save/model.pt\")\n",
    "        \n",
    "        elif i>2000000:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            ac2 = MLPActorCritic(test_env.observation_space, test_env.action_space, **ac_kwargs)\n",
    "            pi_optimizer = Adam(ac2.pi.parameters(), lr=lr)\n",
    "            q_params = itertools.chain(ac2.q1.parameters(), ac2.q2.parameters())\n",
    "            q_optimizer = Adam(q_params, lr=lr)\n",
    "\n",
    "            model_path = \"/checkpoints/model_checkpoint_{}.tar\".format(i)\n",
    "            checkpoint = T.load(dir_path+model_path)\n",
    "            ac2.load_state_dict(checkpoint['ac_state_dict'])\n",
    "            pi_optimizer.load_state_dict(checkpoint['pi_optimizer_state_dict'])\n",
    "            q_optimizer.load_state_dict(checkpoint['q_optimizer_state_dict'])\n",
    "            ac2.eval()\n",
    "\n",
    "        \n",
    "        max_ep_len = 1000\n",
    "\n",
    "        ep_rets = []\n",
    "        soft_ep_rets = []\n",
    "        undisc_ep_rets = []\n",
    "        q1s = []\n",
    "        q2s = []\n",
    "        aveqs = []\n",
    "        \n",
    "        \n",
    "        for j in range(0, 20):\n",
    "            trajs = []\n",
    "            scale = 1.0\n",
    "            gamma = 0.99\n",
    "            o, d, ep_ret, undisc_ep_ret, ep_len = test_env.reset(), False, 0, 0, 0\n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time \n",
    "                prev_o = o\n",
    "                action = ac2.act(T.as_tensor(o, dtype=T.float32), deterministic = True)\n",
    "                \n",
    "                if ep_len == 0:\n",
    "                    q1 = ac2.q1(T.as_tensor(o, dtype=T.float32).view(1,-1), T.as_tensor(action, dtype=T.float32).view(1,-1))\n",
    "                    q1s.append(q1.item())\n",
    "                    q2 = ac2.q2(T.as_tensor(o, dtype=T.float32).view(1,-1), T.as_tensor(action, dtype=T.float32).view(1,-1))\n",
    "                    q2s.append(q2.item())\n",
    "                    aveqs.append((q1.item()+q2.item())/2)\n",
    "                \n",
    "                _, log_prob = ac2.pi(T.as_tensor(o, dtype=T.float32).view(1,-1))\n",
    "                o, r, d, _ = test_env.step(action)\n",
    "                \n",
    "                trajs.append([prev_o, action, o, r, log_prob])\n",
    "                \n",
    "                ep_ret += r*scale\n",
    "                scale *= gamma\n",
    "                \n",
    "                undisc_ep_ret += r\n",
    "                \n",
    "                ep_len += 1\n",
    "                \n",
    "            ep_rets.append(ep_ret)\n",
    "            undisc_ep_rets.append(undisc_ep_ret)\n",
    "\n",
    "            v_tp1 = 0\n",
    "            gamma = 0.99\n",
    "            for t in reversed(trajs):\n",
    "                o, a, next_o, r, log_prob = t\n",
    "                v_t = r + gamma*v_tp1\n",
    "                v_tp1 = v_t - 0.2*log_prob\n",
    "            \n",
    "            soft_ep_rets.append(v_t.item())\n",
    "        \n",
    "        row = pd.Series({'Checkpoint': int(i/10000), 'Return': np.mean(ep_rets), 'Undiscounted Return': np.mean(undisc_ep_rets), \n",
    "                        'Soft Return': np.mean(soft_ep_rets), 'Q1': np.mean(q1s), 'Q2': np.mean(q2s), \n",
    "                         'AvgQ': np.mean(aveqs), 'MSE': (np.mean(soft_ep_rets) - np.mean(aveqs))**2, \n",
    "                         'MAE': np.absolute((np.mean(soft_ep_rets) - np.mean(aveqs)))})\n",
    "        df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        i = i+10000\n",
    "    \n",
    "    df.to_csv(dir_path +'/q_error_wrt_MonteCarlo_estimates.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
